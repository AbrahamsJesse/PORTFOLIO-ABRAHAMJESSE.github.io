                                - *Connecting Python with MySQL Workbench* -

Project Title: Global Tech School

Introduction: This project analyses data from technical schools worldwide, 
having as the primary objective expanding the data pool, comprehending data structures,
creating an SQL database, transferring data from Jupyter to SQL, and performing data cleaning
and manipulation in both Python and SQL.

The primary objectives of this project are as follows:

Adding School IDs: 
-  Incorporate 7 new school IDs into the web scraping code to collect more data 
    that aligns with our project objectives.

-  Comprehend Data Frame Structures: Understand data frame structures using data cleaning 
    and manipulation techniques.

-  Create the New SQL Database: Creating an SQL database named "project_4" to store and 
    manage our collected data.

-  Transferring Data from Jupyter to SQL: Transfer data from Jupyter notebooks to the SQL database.

-  Data Cleaning and Manipulation: Perform data cleaning and manipulation on both Python and SQL to
    prepare the data for analysis.

-  Queries and Insights: Code queries to gain insights and conclusions from the data.


Libraries and Imports:

-  Re: used to perform pattern matching and string manipulation when extracting data from web pages.
    Regular expressions are essential for identifying and capturing specific patterns or information 
    within the text, which is crucial for web scraping.

-  Pandas: it was used to handle and manipulate data. We used DataFrames provided 
    by pandas to structure and organize the collected data for analysis.

-  Json_normalize: the json_normalize function in pandas was utilized to flatten and normalize JSON data.

-  Requests: It was essential for making HTTP requests to websites and APIs, enabling us to retrieve data for
    your project. It facilitated web scraping by fetching web pages and data from online sources.

-  Mysql.connector: The mysql.connector library was used to interact with MySQL databases. This library 
    allowed our team to create a connection to the MySQL database, insert, query, and manage data. 
    It played a key role in storing and retrieving data for analysis.

-  Getpass: It was used for securely inputting database passwords.

- Create_engine from sqlalchemy: was employed to establish a connection to the SQL database.

- Matplotlib.pyplot: It was used for data visualization. This library allowed our group to create charts to visually
    represent my data analysis results, making it easier to convey insights and findings.

Conclusion: 
We have accurately planned a comprehensive data model that allows us to extract valuable 
insights and answers our questions related to technical schools worldwide. This approach allowed 
us to uncover and derive meaningful responses to our specific questions, presenting us with a
well-structured and professionally executed solution for our data analysis needs.




